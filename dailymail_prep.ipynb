{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd005eef6f311b747570f89eafc30ac4b19cb1d6844b383f87a424a5e4387413a6c",
   "display_name": "Python 3.7.10 64-bit ('ml': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "05eef6f311b747570f89eafc30ac4b19cb1d6844b383f87a424a5e4387413a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "BATCH_SIZE = 16 # 16 in orig paper\n",
    "EPOCHS = 3\n",
    "#Tx = 450 #length of input sequence (article)\n",
    "Ty = 50 #length of output sequence (abstract)\n",
    "Tx = 400\n",
    "Ty = 100 # +1 for <eos> token\n",
    "\n",
    "max_global_oov = 100\n",
    "\n",
    "VOCAB_SIZE = len(word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dailymail_word_dict.pickle', 'rb') as f:\n",
    "    word_dict = pickle.load(f)\n",
    "\n",
    "index_dict = {}\n",
    "for key, value in word_dict.items():\n",
    "    index_dict[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, highlights  = tfds.as_numpy(tfds.load(\n",
    "    'cnn_dailymail',\n",
    "    split='validation',\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "287113"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "180.8127772808075"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#creating a dictionary from train\n",
    "word_dict = {}\n",
    "reg = r\"[\\w']+|[.,!?;]\"\n",
    "\n",
    "for i in range(len(articles)):\n",
    "    for word in re.findall(reg, articles[i].decode('UTF-8')) + re.findall(reg, highlights[i].decode('UTF-8')):\n",
    "        word = word.lower()\n",
    "\n",
    "        if word not in word_dict.keys(): word_dict[word] = 0\n",
    "\n",
    "        word_dict[word] += 1\n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "word_dict = {k : v for k, v in sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:vocab_size]} #sorting keys by count of occurences \n",
    "for i, (word, _) in enumerate(word_dict.items()): word_dict[word] = i + 4 #replacing count values by unique token values\n",
    "#special tokens\n",
    "word_dict[\"<sos>\"] = 1 # start of sequence !later used in model\n",
    "word_dict[\"<eos>\"] = 2 # end of sequence\n",
    "word_dict[\"<unk>\"] = 3 # unknown token/word\n",
    "word_dict[\"<pad>\"] = 0 # padding\n",
    "\n",
    "with open('dailymail_word_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(word_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 \t .\n5 \t the\n6 \t ,\n7 \t to\n8 \t a\n9 \t and\n10 \t of\n11 \t in\n12 \t was\n13 \t for\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(word_dict.items())[0:10]: #removing number > x from dict?, jména politiků,\n",
    "    print(value, \"\\t\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy2csv_dataset('dataset_dailymail/val.csv', word_dict, VOCAB_SIZE, Tx, Ty, max_global_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy2csv_dataset(dest_file, word_dict, vocab_size, Tx, Ty, max_global_oov):\n",
    "    \"writes dataset in form X, y, oov_cnt, oov_dict\"\n",
    "    \n",
    "    def word_to_index(word):\n",
    "        if word not in word_dict:\n",
    "            return word_dict[\"<unk>\"]\n",
    "        else: return word_dict[word]\n",
    "\n",
    "    #Ty -= 1 # one token reserved for <eos>\n",
    "\n",
    "    with open(dest_file, 'w') as csv_file:\n",
    "        #writer = csv.DictWriter(csv_file, fieldnames=columns, extrasaction='ignore')\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        for article, highlight in zip(articles, highlights):\n",
    "            oov_cnt = 0\n",
    "            oov_vocab = {} #token index to word\n",
    "            \n",
    "            X_words = re.findall(r\"[\\w']+|[.,!?;]\", article.decode('UTF-8').lower())[:Tx]\n",
    "            y_words = re.findall(r\"[\\w']+|[.,!?;]\", highlight.decode('UTF-8').lower())[:Ty]\n",
    "\n",
    "            for i in range(Tx):\n",
    "                if i < len(X_words):\n",
    "                    index = word_to_index(X_words[i])\n",
    "                    if index == 3:\n",
    "                        if X_words[i] not in oov_vocab and oov_cnt < max_global_oov:\n",
    "                            oov_cnt += 1\n",
    "                            oov_vocab[X_words[i]] = vocab_size + oov_cnt #discarded -1\n",
    "                        \n",
    "                        if X_words[i] in oov_vocab:\n",
    "                            X_words[i] = oov_vocab[X_words[i]]\n",
    "\n",
    "                        else: X_words[i] = index\n",
    "                    else:\n",
    "                        X_words[i] = index\n",
    "                else:\n",
    "                    X_words.append(word_dict[\"<pad>\"])\n",
    "\n",
    "            eos_added = False\n",
    "            for i in range(Ty):\n",
    "                if i < len(y_words):\n",
    "                    index = word_to_index(y_words[i])\n",
    "                    if index == 3 and y_words[i] in oov_vocab:\n",
    "                        y_words[i] = oov_vocab[y_words[i]]\n",
    "                    else:\n",
    "                        y_words[i] = index \n",
    "                elif not eos_added:\n",
    "                    eos_added = True\n",
    "                    y_words.append(word_dict[\"<eos>\"])\n",
    "                else:\n",
    "                    y_words.append(word_dict[\"<pad>\"])\n",
    "            \n",
    "            if not eos_added:\n",
    "                y_words[-1] = word_dict[\"<eos>\"]\n",
    "\n",
    "            #y_words.append(word_dict[\"<eos>\"])\n",
    "\n",
    "            line = X_words + y_words + [oov_cnt]\n",
    "\n",
    "            for key, value in oov_vocab.items():\n",
    "                line.append(key)\n",
    "                line.append(value)\n",
    "            \n",
    "            writer.writerow(line)"
   ]
  }
 ]
}